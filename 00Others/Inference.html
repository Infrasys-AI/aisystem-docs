
<!DOCTYPE html>


<html lang="cn" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="参考链接(DONE)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="00Others/Inference.html" />
<meta property="og:site_name" content="AISystem & AIInfra (AI系统原理)" />
<meta property="og:description" content="参考链接（Reference）介绍了 AI 系统相关的链接。 这里在二次文献中，标注出与一次文献的网络链接关系，实现二次文献与全文的直接链接。 一. AI 系统概述: Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Na..." />
<meta property="og:image:width" content="1146" />
<meta property="og:image:height" content="600" />
<meta property="og:image" content="/_images/social_previews/summary_00Others_Inference_1b34df18.png" />
<meta property="og:image:alt" content="参考链接（Reference）介绍了 AI 系统相关的链接。 这里在二次文献中，标注出与一次文献的网络链接关系，实现二次文献与全文的直接链接。 一. AI 系统概述: Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with..." />
<meta name="description" content="参考链接（Reference）介绍了 AI 系统相关的链接。 这里在二次文献中，标注出与一次文献的网络链接关系，实现二次文献与全文的直接链接。 一. AI 系统概述: Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Na..." />
<meta name="twitter:card" content="summary_large_image" />

    <title>参考链接(DONE) &#8212; AI System</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=362ab14a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="https://assets.readthedocs.org/static/css/readthedocs-doc-embed.css" />
    <link rel="stylesheet" type="text/css" href="https://assets.readthedocs.org/static/css/badge_only.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=a5c4661c" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../_static/documentation_options.js?v=aabdd393"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/rtd-data.js?v=db39d344"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '00Others/Inference';</script>
    <script src="https://assets.readthedocs.org/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="icon" href="../_static/logo-square.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="术语表(DONE)" href="Glossary.html" />
    <link rel="prev" title="本地部署(DONE)" href="Install.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="cn"/>
    <meta name="docbuild:last-update" content="Oct 09, 2024"/>

<link
  rel="alternate"
  type="application/atom+xml"
  href="../reference/blog/atom.xml"
  title="Blog"
/>



  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-wide.svg" class="logo__image only-light" alt="AI System - Home"/>
    <script>document.write(`<img src="../_static/logo-wide.svg" class="logo__image only-dark" alt="AI System - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/chenzomi12/AISystem" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.youtube.com/@ZOMI666" title="Youtube" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-youtube fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">Youtube</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://space.bilibili.com/517221395" title="Blibili" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-bilibili fa-lg" aria-hidden="true"></i></span>
            <span class="sr-only">Blibili</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">=== 一. AI 系统概述 ===</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01Introduction/README.html">课程概述(DONE)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01Introduction/00Introduction.html">本节内容(DONE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../01Introduction/01Present.html">AI 的历史与现状(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01Introduction/02Develop.html">AI 发展驱动力(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01Introduction/03Architecture.html">AI 系统全栈架构(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01Introduction/04Sample.html">AI 系统与程序代码关系(DONE)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">=== 二. AI 硬件体系结构 ===</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02Hardware/README.html">AI 硬件体系架构概述</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware01Foundation/README.html">AI 计算体系概述</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware01Foundation/01Introduction.html">课程内容</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware01Foundation/02ArchSlim.html">AI 计算模式（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware01Foundation/03MobileParallel.html">AI 计算模式（下）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware01Foundation/04Metrics.html">关键设计指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware01Foundation/05Matrix.html">核心计算之矩阵乘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware01Foundation/06BitWidth.html">计算之比特位宽</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware02ChipBase/README.html">AI 芯片基础</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/01CPUBase.html">CPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/02CPUISA.html">CPU 指令集架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/03CPUData.html">CPU 计算本质</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/04CPULatency.html">CPU 计算时延</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/05GPUBase.html">GPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/06NPUBase.html">NPU 基础</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware02ChipBase/07Future.html">超异构计算</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware03GPUBase/README.html">图形处理器 GPU</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware03GPUBase/01Works.html">GPU 工作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware03GPUBase/02Principle.html">为什么 GPU 适用于 AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware03GPUBase/03Concept.html">GPU 架构与 CUDA 关系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware03GPUBase/04History.html">GPU 架构回顾</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware04NVIDIA/README.html">英伟达 GPU 详解</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware04NVIDIA/01BasicTC.html">Tensor Core 基本原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware04NVIDIA/02HistoryTC.html">Tensor Core 架构演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware04NVIDIA/03DeepTC.html">Tensor Core 深度剖析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware04NVIDIA/04BasicNvlink.html">分布式通信与 NVLink</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware04NVIDIA/05DeepNvlink.html">NVLink 原理剖析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware04NVIDIA/06DeepNvswitch.html">NV Switch 深度解析</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware05Abroad/README.html">国外 AI 芯片</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware05Abroad/04TPUIntrol.html">谷歌 TPU 历史发展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware05Abroad/05TPU1.html">谷歌 TPU v1-脉动阵列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware05Abroad/06TPU2.html">谷歌 TPUv2 训练芯片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware05Abroad/07TPU3.html">谷歌 TPUv3 POD 形态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware05Abroad/08TPU4.html">谷歌 TPUv4 与光路交换</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware06Domestic/README.html">国内 AI 芯片</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware06Domestic/04Cambricon.html">寒武纪介绍</a></li>

<li class="toctree-l2"><a class="reference internal" href="../02Hardware06Domestic/08AscendBase.html">昇腾 AI 架构介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware06Domestic/09AscendSOC.html">昇腾 AI 处理器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware06Domestic/10AscendCube.html">昇腾 AI 核心单元</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware06Domestic/11AscendLayout.html">昇腾数据布局转换</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02Hardware07Thought/README.html">AI 芯片黄金十年</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware07Thought/01Introduction.html">芯片的编程体系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware07Thought/02SIMTSIMD.html">SIMD &amp; SIMT 与芯片架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware07Thought/03SPMT.html">SIMD &amp; SIMT 与 CUDA 关系</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware07Thought/04NVSIMT.html">CUDA 编程模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware07Thought/05DSA.html">从 CUDA 对 AI 芯片思考</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02Hardware07Thought/06AIChip.html">AI 芯片的思考</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">=== 三. AI 编程与编译原理 ===</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../03Compiler/README.html">AI 编译原理概述</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03Compiler01Tradition/README.html">传统编译器</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/01Introduction.html">编译器基础介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/02History.html">传统编译器发展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/03GCC.html">GCC 主要特征</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/04LLVM.html">LLVM 架构设计和原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/05LLVMIR.html">LLVM IR 基本概念</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/06LLVMDetail.html">LLVM IR 详解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/07LLVMFrontend.html">LLVM 前端和优化层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler01Tradition/08LLVMBackend.html">LLVM 后端代码生成</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03Compiler02AICompiler/README.html">AI 编译器</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler02AICompiler/01Appear.html">为什么需要 AI 编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler02AICompiler/02Stage.html">AI 编译器历史阶段</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler02AICompiler/03Architecture.html">AI 编译器基本架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler02AICompiler/04Future.html">AI 编译器挑战与思考</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03Compiler03Frontend/README.html">前端优化</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/01Introduction.html">AI 编译器前端优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/02GraphIR.html">图算 IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/03OPFusion.html">算子融合</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/04LayoutPrinc.html">布局转换原理与算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/06Memory.html">内存分配算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/07ConstantFold.html">常量折叠原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/08CSE.html">公共表达式消除原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/09DCE.html">死代码消除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler03Frontend/10algebraic.html">代数简化</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03Compiler04Backend/README.html">后端优化</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/01Introduction.html">AI 编译器后端优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/02OPSCompute.html">计算与调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/03Optimization.html">算子手工优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/04LoopOpt.html">算子循环优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/05OtherOpt.html">指令和存储优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/06AutoTuning.html">Auto-Tuning 原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler04Backend/07Practice.html">TVM 实践案例</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03Compiler07CANN/README.html">CANN &amp; Ascend C</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler07CANN/01CANN.html">昇腾异构计算架构 CANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler07CANN/02OPType.html">CANN 算子类型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler07CANN/03AscendC.html">算子开发编程语言 Ascend C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler07CANN/04Grmmar.html">Ascend C 语法扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03Compiler07CANN/05Paradigm.html">Ascend C 编程范式</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">=== 四. 推理系统&amp;引擎 ===</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../04Inference/README.html">推理系统&amp;引擎概述</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04Inference01Inference/README.html">推理系统</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/01Introduction.html">引言</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/02Constraints.html">推理系统介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/03Workflow.html">推理流程全景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/04System.html">推理系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/05Inference.html">推理引擎架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/07MindIEIntro.html">昇腾推理引擎 MindIE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference01Inference/08AscendCL.html">推理引擎示例：AscendCL</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04Inference02Mobilenet/README.html">模型轻量化</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/01Introduction.html">推理参数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/021Squeezenet.html">SqueezeNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/022Shufflenet.html">ShuffleNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/023Mobilenet.html">MobileNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/024ESPNet.html">ESPNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/025FBNet.html">FBNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/026EfficientNet.html">EfficientNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/027GhostNet.html">GhostNet 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/02Cnn.html">CNN 模型小型化（上）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/031MobileVit.html">MobileVit 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/032MobileFormer.html">MobileFormer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/033EfficientFormer.html">EfficientFormer 系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/03Cnn.html">CNN 模型小型化（下）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/03Transformer.html">Transformer 小型化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference02Mobilenet/04Transformer.html">Transformer 模型小型化</a></li>

</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04Inference03Slim/README.html">模型压缩</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04Inference03Slim/01Introduction.html">基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference03Slim/02Quant.html">低比特量化原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference03Slim/03QAT.html">感知量化训练 QAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference03Slim/04PTQ.html">训练后量化与部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference03Slim/05Pruning.html">模型剪枝</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference03Slim/06Distillation.html">知识蒸馏原理</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04Inference04Converter/README.html">模型转换</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04Inference04Converter/01Introduction.html">基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference04Converter/02Principle.html">推理文件格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference04Converter/03IR.html">自定义计算图 IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference04Converter/04Detail.html">模型转换流程</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04Inference05Optimize/README.html">模型优化</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04Inference05Optimize/01Optimizer.html">计算图优化架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference05Optimize/02Basic.html">离线图优化技术</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference05Optimize/03Extend.html">其他计算图优化</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04Inference06Kernel/README.html">Kernel 优化</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/01Introduction.html">Kernel 层架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/02Conv.html">卷积操作原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/03Im2col.html">Im2Col 算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/04Winograd.html">Winograd 算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/05Qnnpack.html">QNNPack 算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/06Memory.html">推理内存布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04Inference06Kernel/08Others.html">汇编与循环优化</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">=== 五. AI 框架核心模块 ===</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../05Framework/README.html">AI 框架核心概述</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../05Framework01Foundation/README.html">AI 框架基础</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../05Framework01Foundation/01Introduction.html">内容介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework01Foundation/02Fundamentals.html">AI 框架作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework01Foundation/03History.html">AI 框架之争</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework01Foundation/04Programing.html">框架编程范式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework01Foundation/05MindSpore.html">昇思 MindSore 关键特性</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../05Framework02AutoDiff/README.html">自动微分</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/01Introduction.html">自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/02BaseConcept.html">什么是微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/03GradMode.html">微分计算模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/04Implement.html">微分实现方式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/05ForwardMode.html">动手实现自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/06ReversedMode.html">动手实现 PyTorch 微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework02AutoDiff/07Challenge.html">自动微分的挑战&amp;未来</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../05Framework03DataFlow/README.html">计算图</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/01Introduction.html">基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/02Computegraph.html">计算图原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/03Atuodiff.html">计算图与自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/04Dispatch.html">计算图的调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/05ControlFlow.html">计算图的控制流实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/06StaticGraph.html">动态图与静态图转换</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework03DataFlow/07Future.html">计算图挑战与未来</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../05Framework04Parallel/README.html">分布式并行</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/01Introduction.html">基本介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/02DataParallel.html">数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/03ZeRODP.html">数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/04TensorParallel.html">张量并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/05PipelineParallel.html">流水并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/06HybridParallel.html">混合并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../05Framework04Parallel/07MSParallel.html">昇思MindSpore并行</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">=== 附录内容 ===</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="README.html">附录(DONE)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Editors.html">编辑和作者(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Instruments.html">书写工具(DONE)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="Install.html">本地部署(DONE)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">参考链接(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Glossary.html">术语表(DONE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Criterion.html">书写规范(DONE)</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/chenzomi12/chenzomi12.github.io/blob/master/00Others/Inference.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/chenzomi12/chenzomi12.github.io/edit/master/00Others/Inference.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/chenzomi12/chenzomi12.github.io/issues/new?title=Issue%20on%20page%20%2F00Others/Inference.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/00Others/Inference.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>参考链接(DONE)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">一. AI 系统概述</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">二. AI 硬件体系结构</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">三. AI 编译器</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">四. 推理系统&amp;引擎</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">五. AI 框架核心模块</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
<!--Copyright © ZOMI 适用于[License](https://github.com/chenzomi12/AISystem)版权许可-->
<section class="tex2jax_ignore mathjax_ignore" id="done">
<h1>参考链接(DONE)<a class="headerlink" href="#done" title="Link to this heading">#</a></h1>
<p>参考链接（Reference）介绍了 AI 系统相关的链接。</p>
<blockquote>
<div><p>这里在二次文献中，标注出与一次文献的网络链接关系，实现二次文献与全文的直接链接。</p>
</div></blockquote>
<section id="ai">
<h2>一. AI 系统概述<a class="headerlink" href="#ai" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.nature.com/articles/nature16961">Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016). https://doi.org/10.1038/nature16961</a></p></li>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007/BF02478259">McCulloch, W.S., Pitts, W. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5, 115–133 (1943).</a></p></li>
<li><p><a class="reference external" href="https://bibbase.org/network/publication/rosenblatt-theperceptronaperceivingandrecognizingautomaton-1957">The perceptron - A perceiving and recognizing automaton. Rosenblatt, F. Technical Report 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New York, January, 1957.</a></p></li>
<li><p><a class="reference external" href="https://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf">Bernard Widrow. (1960). “Adaptive &quot;Adaline&quot; Neuron Using Chemical &quot;memistors&quot;.” Number Technical Report 1553-2. Stanford Electron. Labs. Stanford, CA</a></p></li>
<li><p><a class="reference external" href="https://www.amazon.com/Perceptrons-Introduction-Computational-Geometry-Expanded/dp/0262631113">Minsky, M., Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. Cambridge, MA, USA: MIT Press.</a></p></li>
<li><p><a class="reference external" href="https://books.xn--flw351e.com/books/about/Beyond_Regression.html?id=z81XmgEACAAJ">Werbos, Paul J.. “Beyond Regression : &quot;New Tools for Prediction and Analysis in the Behavioral Sciences.” (1974).</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/abs/10.5555/2887770.2887799">Rina Dechter. 1986. Learning while searching in constraint-satisfaction-problems. In Proceedings of the Fifth AAAI National Conference on Artificial Intelligence (AAAI'86). AAAI Press, 178–183.</a></p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/document/6795724">Y. LeCun et al., &quot;Backpropagation Applied to Handwritten Zip Code Recognition,&quot; in Neural Computation, vol. 1, no. 4, pp. 541-551, Dec. 1989, doi: 10.1162/neco.1989.1.4.541.</a></p></li>
<li><p><a class="reference external" href="https://www.science.org/doi/10.1126/science.1127647">Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006 Jul 28;313(5786):504-7. doi: 10.1126/science.1127647. PMID: 16873662.</a></p></li>
<li><p><a class="reference external" href="https://image-net.org/">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255).
</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.5555/3042573.3042574">Dong Yu, Frank Seide, and Gang Li. 2012. Conversational speech transcription using context-dependent deep neural networks. In Proceedings of the 29th International Coference on International Conference on Machine Learning (ICML'12). Omnipress, Madison, WI, USA, 1–2.</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.5555/3042573.3042641">Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng. 2012. Building high-level features using large scale unsupervised learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning (ICML'12). Omnipress, Madison, WI, USA, 507–514.</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.1145/3065386">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017. ImageNet classification with deep convolutional neural networks. Commun. ACM 60, 6 (June 2017), 84–90. https://doi.org/10.1145/3065386</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.5555/3454287.3455008">Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: an imperative style, high-performance deep learning library. Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 721, 8026–8037.</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.5555/3026877.3026899">Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: a system for large-scale machine learning. In Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation (OSDI'16). USENIX Association, USA, 265–283.</a></p></li>
</ol>
</section>
<section id="id1">
<h2>二. AI 硬件体系结构<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>https://www.knime.com/blog/a-friendly-introduction-to-deep-neural-networks</p></li>
<li><p>https://machine-learning.paperspace.com/wiki/activation-function</p></li>
<li><p>https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/</p></li>
<li><p>https://arxiv.org/pdf/1704.04861
英伟达 GPU 架构白皮书：<a class="reference external" href="https://www.NVIDIA.cn/technologies/">https://www.NVIDIA.cn/technologies/</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1704.04760">In-Datacenter Performance Analysis of a Tensor Processing Unit</a></p></li>
<li><p>[An in-depth look at 谷歌’s first Tensor Processing Unit (TPU)](https://cloud.谷歌.com/blog/products/ai-machine-learning/an-in-depth-look-at-谷歌 s-first-tensor-processing-unit-tpu)</p></li>
<li><p><a class="reference external" href="https://blog.xn--flw351e/products/pixel/%E8%B0%B7%E6%AD%8C-tensor-g3-pixel-8/">谷歌 Tensor G3: The new chip that gives your Pixel an AI upgrade</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit#cite_note-TPU_memory-15">Wikipedia-Tensor Processing Unit</a></p></li>
<li><p><a class="reference external" href="https://cacm.acm.org/research/a-domain-specific-supercomputer-for-training-deep-neural-networks/">A Domain-Specific Supercomputer for Training Deep Neural Networks</a></p></li>
</ol>
<p>[1] Chen T , Du Z , Sun N ,et al.DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning[C]//International Conference on Architectural Support for Programming Languages &amp; Operating Systems.ACM, 2014.DOI:10.1145/2541940.2541967.</p>
<p>[2] Chen Y , Luo T , Liu S ,et al.DaDianNao: A Machine-Learning Supercomputer[C]//2014 47th Annual IEEE/ACM International Symposium on Microarchitecture.0[2024-04-14].DOI:10.1109/MICRO.2014.58.</p>
<p>[3] Du, Z., Fasthuber, R., Chen, T., Ienne, P., Li, L., Luo, T., Feng, X., Chen, Y., Temam, O., 2015. ShiDianNao: shifting vision processing closer to the sensor, in: Proceedings of the 42nd Annual International Symposium on Computer Architecture. Presented at the ISCA ’15: The 42nd Annual International Symposium on Computer Architecture, ACM, Portland Oregon, pp. 92–104. https://doi.org/10.1145/2749469</p>
<p>[4] Liu, D., Chen, T., Liu, S., Zhou, J., Zhou, S., Teman, O., Feng, X., Zhou, X., Chen, Y., 2015. PuDianNao: A Polyvalent Machine Learning Accelerator, in: Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems. Presented at the ASPLOS ’15: Architectural Support for Programming Languages and Operating Systems, ACM, Istanbul Turkey, pp. 369–381. https://doi.org/10.1145/2694344</p>
<p>[5] Liu S, Du Z, Tao J, et al. Cambricon: An Instruction Set Architecture for Neural Networks[C]// Acm/ieee International Symposium on Computer Architecture. 2016.</p>
<p>[6] <a class="reference external" href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/programming_guide_1.7.0/hardware_implementation/index.html#">寒武纪 CAMBRICON BANG C/C++ 编程指南</a></p>
<p>[7] 陈云霁，李玲，李威，郭崎，杜子东，2020. 《智能计算系统》, 机械工业出版社</p>
<p>[1] 未名超算队. &quot;北大未名超算队高性能计算入门讲座（一）:概论.&quot; Bilibili, [https://www.bilibili.com/video/BV1814y1g7YC/]</p>
<p>[2] 专用架构与 AI 软件栈（1）. Zhihu, [https://zhuanlan.zhihu.com/p/387269513]</p>
<p>[3] &quot;AMD’s CDNA 3 Compute Architecture.&quot; Chips and Cheese, [https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/]</p>
<p>[4] CUDA 生态才是英伟达 AI 霸主护城河-深度分析 2024. WeChat, [https://mp.weixin.qq.com/s/VGej8Jjags5v0JsHIuf_tQ]</p>
<p>[1] 未名超算队. &quot;北大未名超算队高性能计算入门讲座（一）:概论.&quot; Bilibili, [https://www.bilibili.com/video/BV1814y1g7YC/]</p>
<p>[2] 专用架构与 AI 软件栈（1）. Zhihu, [https://zhuanlan.zhihu.com/p/387269513]</p>
<p>[3] &quot;AMD’s CDNA 3 Compute Architecture.&quot; Chips and Cheese, [https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/]</p>
<p>[4] CUDA 生态才是英伟达 AI 霸主护城河-深度分析 2024. WeChat, [https://mp.weixin.qq.com/s/VGej8Jjags5v0JsHIuf_tQ]</p>
<p>[1] &quot;David Patterson: A Decade of Machine Learning Accelerators:Lessons Learned and Carbon Footprint&quot; YouTube, [https://www.youtube.com/watch?v=PLK3pGELbSs]</p>
<p>[2] &quot;TPU 演进十年：谷歌的十大经验教训&quot; 知乎, [https://zhuanlan.zhihu.com/p/573794328]</p>
</section>
<section id="id2">
<h2>三. AI 编译器<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>此外，c 语言中常见的操作还有对数组和结构体的操作，内置函数和外部函数的引用等，更深一步的内容可以参考<a class="reference external" href="https://blog.csdn.net/qq_42570601/article/details/107157224">简单了解 LLVM IR 基本语法-CSDN 博客</a></p>
<ol class="arabic simple">
<li><p>https://zh.wikipedia.org/wiki/三位址碼</p></li>
<li><p>https://buaa-se-compiling.github.io/miniSysY-tutorial/pre/llvm_ir_quick_primer.html</p></li>
<li><p>https://llvm-tutorial-cn.readthedocs.io/en/latest/chapter-2.html</p></li>
<li><p>https://buaa-se-compiling.github.io/miniSysY-tutorial/pre/llvm_ir_ssa.html</p></li>
<li><p>https://buaa-se-compiling.github.io/miniSysY-tutorial/pre/design_hints.html</p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/qq_42570601/article/details/107157224">简单了解 LLVM IR 基本语法-CSDN 博客</a></p></li>
<li><p>https://learning.acm.org/techtalks/computerarchitecture</p></li>
<li><p>https://segmentfault.com/a/1190000041739045</p></li>
</ol>
</section>
<section id="id3">
<h2>四. 推理系统&amp;引擎<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1811.09886">Deep Learning Inference in Meta Data Centers: Characterization, Performance Optimizations and Hardware Implications</a></p></li>
<li><p><a class="reference external" href="https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf">Clipper: A Low-Latency Online Prediction Serving System</a></p></li>
<li><p><a class="reference external" href="https://www.kdd.org/kdd2017/papers/view/tfx-a-TensorFlow-based-production-scale-machine-learning-platform">TFX: A TensorFlow-Based Production-Scale Machine Learning Platform</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1712.06139">TensorFlow-Serving: Flexible, High-Performance ML Serving</a></p></li>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf">Optimal Aggregation Policy for Reducing Tail Latency of Web Search</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1710.09282.pdf">A Survey of Model Compression and Acceleration for Deep Neural Networks</a></p></li>
<li><p><a class="reference external" href="http://dlsys.cs.washington.edu/pdf/lecture12.pdf">CSE 599W: System for ML - Model Serving</a></p></li>
<li><p><a class="reference external" href="https://developer.NVIDIA.com/deep-learning-performance-training-inference">https://developer.NVIDIA.com/deep-learning-performance-training-inference</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1510.00149.pdf">DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</a></p></li>
<li><p><a class="reference external" href="https://pdfs.semanticscholar.org/1ff9/a37d766e3a4f39757f5e1b235a42dacf18ff.pdf">Learning both Weights and Connections for Efficient Neural Networks</a></p></li>
<li><p><a class="reference external" href="http://on-demand.GPUtechconf.com/gtcdc/2017/presentation/dc7172-shashank-prasanna-deep-learning-deployment-with-NVIDIA-tensorrt.pdf">DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT</a></p></li>
<li><p><a class="reference external" href="https://people.csail.mit.edu/jrk/halide-pldi13.pdf">Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines</a></p></li>
<li><p><a class="reference external" href="https://www.usenix.org/system/files/osdi18-chen.pdf">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></p></li>
<li><p><a class="reference external" href="http://on-demand.GPUtechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">8-bit Inference with TensorRT</a></p></li>
<li><p><a class="github reference external" href="https://github.com/microsoft/AI-System">microsoft/AI-System</a></p></li>
<li><p><a class="reference external" href="https://chenzomi12.github.io/040Inference/README.html">推理系统&amp;引擎</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/423551635">NCNN、OpenVino、 TensorRT、MediaPipe、ONNX，各种推理部署架构，到底哪家强？</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/665146747">【AI System】第 8 章：深度学习推理系统</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_45651194/article/details/132872588">【AI】推理系统和推理引擎的整体架构</a></p></li>
<li><p>Deep Learning Inference in Meta Data Centers: Characterization, Performance Optimizations and Hardware Implications</p></li>
<li><p>Clipper: A Low-Latency Online Prediction Serving System</p></li>
<li><p>TFX: A TensorFlow-Based Production-Scale Machine Learning Platform</p></li>
<li><p>TensorFlow-Serving: Flexible, High-Performance ML Serving</p></li>
<li><p>Optimal Aggregation Policy for Reducing Tail Latency of Web Search</p></li>
<li><p>A Survey of Model Compression and Acceleration for Deep Neural Networks</p></li>
<li><p>CSE 599W: System for ML - Model Serving</p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/deep-learning-performance-training-inference">Deep Learning Performance Training Inference</a></p></li>
<li><p>DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</p></li>
<li><p>Learning both Weights and Connections for Efficient Neural Networks</p></li>
<li><p>DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT</p></li>
<li><p>Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines</p></li>
<li><p>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</p></li>
<li><p>8-bit Inference with TensorRT</p></li>
<li><p><a class="reference external" href="https://github.com/microsoft/AI-System">Microsoft AI System</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/354058294">模型推理服务化之 Triton：如何基于 Triton 开发自己的推理引擎？</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1811.09886">Deep Learning Inference in Meta Data Centers: Characterization, Performance Optimizations and Hardware Implications</a></p></li>
<li><p><a class="reference external" href="https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf">Clipper: A Low-Latency Online Prediction Serving System</a></p></li>
<li><p><a class="reference external" href="https://www.kdd.org/kdd2017/papers/view/tfx-a-TensorFlow-based-production-scale-machine-learning-platform">TFX: A TensorFlow-Based Production-Scale Machine Learning Platform</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1712.06139">TensorFlow-Serving: Flexible, High-Performance ML Serving</a></p></li>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/samehe-2015sigir.optimalaggregation.pdf">Optimal Aggregation Policy for Reducing Tail Latency of Web Search</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1710.09282.pdf">A Survey of Model Compression and Acceleration for Deep Neural Networks</a></p></li>
<li><p><a class="reference external" href="http://dlsys.cs.washington.edu/pdf/lecture12.pdf">CSE 599W: System for ML - Model Serving</a></p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/deep-learning-performance-training-inference">https://developer.nvidia.com/deep-learning-performance-training-inference</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1510.00149.pdf">DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</a></p></li>
<li><p><a class="reference external" href="https://pdfs.semanticscholar.org/1ff9/a37d766e3a4f39757f5e1b235a42dacf18ff.pdf">Learning both Weights and Connections for Efficient Neural Networks</a></p></li>
<li><p><a class="reference external" href="http://on-demand.GPUtechconf.com/gtcdc/2017/presentation/dc7172-shashank-prasanna-deep-learning-deployment-with-nvidia-tensorrt.pdf">DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT</a></p></li>
<li><p><a class="reference external" href="https://people.csail.mit.edu/jrk/halide-pldi13.pdf">Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines</a></p></li>
<li><p><a class="reference external" href="https://www.usenix.org/system/files/osdi18-chen.pdf">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></p></li>
<li><p><a class="reference external" href="http://on-demand.GPUtechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">8-bit Inference with TensorRT</a></p></li>
<li><p><a class="github reference external" href="https://github.com/microsoft/AI-System">microsoft/AI-System</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/665146747">【AI System】第 8 章：深度学习推理系统</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/qq_21370465/article/details/109740949">Tengine-Kit 人脸检测及关键点</a></p></li>
<li><p><a class="reference external" href="https://segmentfault.com/a/1190000037710505">Crazy Rockets-教你如何集成华为 HMS ML Kit 人脸检测和手势识别打造爆款小游戏</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/465623148">记录自己神经网络模型训练的全流程</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_45651194/article/details/132872588">推理系统和推理引擎的整体架构</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_44533869/article/details/125223704">Pytorch-Onnx-Tensorrt 模型转换教程案例</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r2.3.0rc2/beginner/introduction.html">昇思 MindSpore 基本介绍</a></p></li>
<li><p><a class="reference external" href="https://www.paddlepaddle.org.cn/overview">飞桨产品全景</a></p></li>
<li><p>Deep Learning Inference in Meta Data Centers: Characterization, Performance Optimizations and Hardware Implications</p></li>
<li><p>Clipper: A Low-Latency Online Prediction Serving System</p></li>
<li><p>TFX: A TensorFlow-Based Production-Scale Machine Learning Platform</p></li>
<li><p>TensorFlow-Serving: Flexible, High-Performance ML Serving</p></li>
<li><p>Optimal Aggregation Policy for Reducing Tail Latency of Web Search</p></li>
<li><p>A Survey of Model Compression and Acceleration for Deep Neural Networks</p></li>
<li><p>CSE 599W: System for ML - Model Serving</p></li>
<li><p>https://developer.NVIDIA.com/deep-learning-performance-training-inference</p></li>
<li><p>DEEP COMPRESSION:   COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</p></li>
<li><p>Learning both Weights and Connections for Efficient Neural Networks</p></li>
<li><p>DEEP LEARNING DEPLOYMENT WITH NVIDIA TENSORRT</p></li>
<li><p>Halide: A Language and Compiler for Optimizing Parallelism,Locality, and Recomputation in Image Processing Pipelines</p></li>
<li><p>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</p></li>
<li><p>8-bit Inference with TensorRT</p></li>
<li><p>https://github.com/microsoft/AI-System</p></li>
<li><p>J. Mao, X. Chen, K. W. Nixon, C. Krieger, and Y. Chen, “MoDNN: Local distributed mobilecomputing system for deep neural network,” in Proc. Design, Autom. Test Eur. Conf. Exhibit.(DATE), Mar. 2017, pp. 1396–1401.</p></li>
<li><p>Z. Zhao, K. M. Barijough, and A. Gerstlauer, “Deepthings: Distributed adaptive deep learning inference on resource-constrained iot edge clusters,” IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 37, no. 11, pp. 2348–2359, Nov. 2018.</p></li>
</ol>
<p>1.<a class="reference external" href="https://dl.acm.org/doi/10.1145/3065386">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In NIPS, 2012</a></p>
<p>2.<a class="reference external" href="https://dl.acm.org/doi/10.1145/3065386">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017. ImageNet classification with deep convolutional neural networks. Commun. ACM 60, 6 (June 2017), 84–90. https://doi.org/10.1145/3065386</a></p>
<p>3.<a class="reference external" href="https://www.nature.com/articles/nature16961">Silver, D., Huang, A., Maddison, C. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016). https://doi.org/10.1038/nature16961</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/abs/1409.1556">Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1807.06434">Mohamed S Abdelfattah, David Han, Andrew Bitar, Roberto DiCecco, Shane O’Connell,Nitika Shanker, Joseph Chu, Ian Prins, Joshua Fender, Andrew C Ling, et al. Dla: Compiler and fpga overlay for neural network inference acceleration. In International Conference on Field Programmable Logic and Applications, pages 411–4117. IEEE, 2018.</a></p>
<p>1.<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf">François Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv:1610.02357, 2016.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/abs/1611.05431">Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. arXiv:1611.05431, 2016.</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/abs/1512.03385">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/pdf/1704.04861.pdf%EF%BC%89">Howard, Andrew G., et al. &quot;Mobilenets: Efficient convolutional neural networks for mobile vision applications.&quot; arXiv preprint arXiv:1704.04861 (2017).</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1506.01497">Ren, Shaoqing, et al. &quot;Faster R-CNN: Towards real-time object detection with region proposal networks.&quot; Advances in neural information processing systems. 2015.</a></p>
<p>6.<a class="reference external" href="https://arxiv.org/abs/1708.06519">Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolutional networks through network slimming. In: 2017 IEEE International Conference on Computer Vision (ICCV), IEEE (2017) 2755–2763</a></p>
<p>7.<a class="reference external" href="https://ieeexplore.ieee.org/document/7298809/">Zhang, X., Zou, J., Ming, X., He, K., Sun, J.: Efficient and accurate approximations of nonlinear convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2015) 1984–1992</a></p>
<p>8.<a class="reference external" href="https://ieeexplore.ieee.org/document/7332968/">Zhang, X., Zou, J., He, K., Sun, J.: Accelerating very deep convolutional networks for classification and detection. IEEE transactions on pattern analysis and machine intelligence 38(10) (2016) 1943–1955</a></p>
<p>9.<a class="reference external" href="https://arxiv.org/pdf/1410.0759v3.pdf">Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B.,Shelhamer, E.: cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014)</a></p>
<p>10.<a class="reference external" href="https://arxiv.org/pdf/1602.06709.pdf">Das, D., Avancha, S., Mudigere, D., Vaidynathan, K., Sridharan, S., Kalamkar,D., Kaul, B., Dubey, P.: Distributed deep learning using synchronous stochastic gradient descent. arXiv preprint arXiv:1602.06709 (2016)</a></p>
<p>11.<a class="reference external" href="https://arxiv.org/abs/1605.06489v1">Ioannou, Y., Robertson, D., Cipolla, R., Criminisi, A.: Deep roots: Improving cnn efficiency with hierarchical filter groups. arXiv preprint arXiv:1605.06489 (2016)</a></p>
<p>12.<a class="reference external" href="https://arxiv.org/abs/1707.02725">Zhang, T., Qi, G.J., Xiao, B., Wang, J.: Interleaved group convolutions for deep neural networks. In: International Conference on Computer Vision. (2017)</a></p>
<p>13.<a class="reference external" href="https://arxiv.org/abs/1804.06202">Xie, G., Wang, J., Zhang, T., Lai, J., Hong, R., Qi, G.J.: Igcv 2: Interleaved structured sparse convolutional neural networks. arXiv preprint arXiv:1804.06202(2018)</a></p>
<p>14.<a class="reference external" href="https://arxiv.org/abs/1806.00178v2">Sun, K., Li, M., Liu, D., Wang, J.: Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks. arXiv preprint arXiv:1806.00178 (2018)</a></p>
<p>15.<a class="reference external" href="https://arxiv.org/abs/1602.07261">Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: AAAI. Volume 4. (2017)</a></p>
<p>16.<a class="reference external" href="https://arxiv.org/abs/1505.06798">Zhang, X., Zou, J., He, K., Sun, J.: Accelerating very deep convolutional networks for classification and detection. IEEE transactions on pattern analysis and machine  intelligence 38(10) (2016) 1943–1955</a></p>
<p>17.<a class="reference external" href="https://arxiv.org/pdf/1410.0759v3.pdf">Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B.,Shelhamer, E.: cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014)</a></p>
<p>18.<a class="reference external" href="https://arxiv.org/abs/1409.0575">O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,et al. Imagenet large scale visual recognition challenge.International Journal of Computer Vision, 115(3):211–252,2015.</a></p>
<p>19.<a class="reference external" href="https://arxiv.org/abs/1606.06160">S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 2</a></p>
<p>20.<a class="reference external" href="https://arxiv.org/abs/1707.07012v1">B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable image recognition. arXiv preprint arXiv:1707.07012, 2017. 1,</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/abs/1603.04467">M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.TensorFlow: Large-scale machine learning on heterogeneous  systems, 2015. Software available from TensorFlow. org, 1,2015.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/pdf/1609.07061.pdf">I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061, 2016. 2</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/pdf/1602.07360.pdf">F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J.Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 1mb model size. arXiv preprint arXiv:1602.07360, 2016. 1, 6</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/abs/1502.03167">S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.arXiv preprint arXiv:1502.03167, 2015.</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1405.3866">M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions.arXiv preprint arXiv:1405.3866, 2014. 2</a></p>
<p>6.<a class="reference external" href="https://dl.acm.org/doi/10.1145/2647868.2654889">Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014. 4</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/abs/1412.5474v4"> J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional neural networks for feedforward acceleration. arXiv preprint arXiv:1412.5474, 2014. 1, 3</a></p>
<p>8.<a class="reference external" href="http://vision.stanford.edu/aditya86/ImageNetDogs">A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei.Novel dataset for fine-grained image categorization. In First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition,Colorado Springs, CO, June 2011. 6</a></p>
<p>9.<a class="reference external" href="https://arxiv.org/abs/1511.06789v1">J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable effectiveness of noisy data for fine-grained recognition. arXiv preprint arXiv:1511.06789, 2015. 6</a></p>
<p>10.<a class="reference external" href="https://www.scitepress.org/Papers/2019/74696/74696.pdf">R. Avenash and P. Vishawanth. Semantic segmentation of satellite images using a modified cnn with hard-swish activation function. In VISIGRAPP, 2019. 2, 4</a></p>
<p>11.<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html"> Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi,Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR,2017. 7</a></p>
<p>12.Wei Liu, Dragomir Anguelov, Dumitru Erhan,Christian Szegedy, Scott Reed, Cheng-Yang Fu,and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016.</p>
<p>13.<a class="reference external" href="https://arxiv.longhoe.net/abs/1512.02325">Jonathan Huang, Vivek Rathod, Derek Chow,Chen Sun, and Menglong Zhu. TensorFlow object detection api, 2017. 7</a></p>
<p>14.<a class="reference external" href="https://arxiv.org/abs/1706.05587">Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017. 7</a></p>
<p>15.<a class="reference external" href="https://cir.nii.ac.jp/crid/1573668925020519424">Matthias Holschneider, Richard KronlandMartinet, Jean Morlet, and Ph Tchamitchian.A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets:Time-Frequency Methods and Phase Space, pages 289–297. 1989. 7</a></p>
<p>16.<a class="reference external" href="https://cir.nii.ac.jp/crid/1573668925020519424">Pierre Sermanet, David Eigen, Xiang Zhang,Michael Mathieu, Rob Fergus, and Yann Le- ¨Cun. Overfeat: Integrated recognition, localization and detection using convolutional networks.arXiv:1312.6229, 2013. 7</a></p>
<p>17.<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Papandreou_Modeling_Local_and_2015_CVPR_paper.html">George Papandreou, Iasonas Kokkinos, and PierreAndre Savalle. Modeling local and global deformations in deep learning: Epitomic convolution,multiple instance learning, and sliding window detection. In CVPR, 2015. 7</a></p>
<p>18.<a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2014arXiv1405.0312L/abstract">T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 7</a></p>
<p>19<a class="reference external" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper.html">C. Liu, B. Zoph, J. Shlens, W. Hua, L. Li, L. Fei-Fei, A. L.Yuille, J. Huang, and K. Murphy.Progressive neural architecture search. CoRR, abs/1712.00559, 2017. 2</a></p>
<p>20.<a class="reference external" href="https://arxiv.longhoe.net/abs/1806.09055">H. Liu, K. Simonyan, and Y. Yang. DARTS: differentiable architecture search. CoRR, abs/1806.09055, 2018. 2</a></p>
<p>21.<a class="reference external" href="https://arxiv.longhoe.net/abs/1506.04579">W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking wider to see better. CoRR, abs/1506.04579, 2015. 7</a></p>
<p>22.<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html"> J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 8</a></p>
<p>22.<a class="reference external" href="https://arxiv.longhoe.net/abs/1803.06815">S. Mehta, M. Rastegari, A. Caspi, L. G. Shapiro, and H. Hajishirzi. Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation. In Computer Vision -ECCV 2018 - 15th European Conference, Munich, Germany,September 8-14, 2018, Proceedings, Part X, pages 561–580,2018. 8</a></p>
<p>23.<a class="reference external" href="https://arxiv.longhoe.net/abs/1811.11431">S. Mehta, M. Rastegari, L. G. Shapiro, and H. Hajishirzi. Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network. CoRR, abs/1811.11431, 2018.</a></p>
<p>24.<a class="reference external" href="https://www.researchgate.net/publication/329607971_Concentrated-Comprehensive_Convolutions_for_lightweight_semantic_segmentation">H. Park, Y. Yoo, G. Seo, D. Han, S. Yun, and N. Kwak.Concentrated-comprehensive convolutions for lightweightsemantic segmentation. CoRR, abs/1812.04920, 2018. 8</a></p>
<p>25.<a class="reference external" href="https://arxiv.longhoe.net/abs/1802.03268">H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean.Efficient neural architecture search via parameter sharing.CoRR, abs/1802.03268, 2018. 2</a></p>
<p>26.<a class="reference external" href="https://arxiv.longhoe.net/abs/1710.05941">P. Ramachandran, B. Zoph, and Q. V. Le. Searching for activation functions. CoRR, abs/1710.05941, 2017. 2, 4</a></p>
<p>27.<a class="reference external" href="https://arxiv.longhoe.net/abs/1602.07360">F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J.Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy  with 50x fewer parameters and &lt;1mb model size. CoRR,abs/1602.07360, 2016. 2</a></p>
<p>28.<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Quantized_Convolutional_Neural_CVPR_2016_paper.html">J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng. Quantized convolutional neural networks for mobile devices. CoRR,abs/1512.06473, 2015. 2</a></p>
<p>29.<a class="reference external" href="https://arxiv.longhoe.net/abs/1606.06160">S. Zhou, Z. Ni, X. Zhou, H. Wen, Y. Wu, and Y. Zou. Dorefanet: Training low bitwidth convolutional neural networks with low bitwidth gradients. CoRR, abs/1606.06160, 2016.</a></p>
<p>30.<a class="reference external" href="https://arxiv.org/pdf/2303.14189.pdf">Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: A fast hybrid vision transformer using structural reparameterization. arXiv preprint arXiv:2303.14189, 2023.</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/pdf/1612.01105.pdf">Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: CVPR. (2017)</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/abs/1406.4729">He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks for visual recognition. In: ECCV. (2014)</a></p>
<p>3.<a class="reference external" href="https://www.bmva.org/bmvc/2009/Papers/Paper350/Abstract350.pdf">Ess, A., Muller, T., Grabner, H., Van Gool, L.J.: Segmentation-based urban traffic scene understanding. In: BMVC. (2009)</a></p>
<p>4.<a class="reference external" href="https://ieeexplore.ieee.org/document/7298925/">Menze, M., Geiger, A.: Object scene flow for autonomous vehicles. In: CVPR. (2015)</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1703.03098v1">Xiang, Y., Fox, D.: DA-RNN: Semantic mapping with data associated recurrent neural networks. Robotics: Science and Systems (RSS) (2017)</a></p>
<p>6.<a class="reference external" href="https://arxiv.org/abs/1610.02357">Chollet, F.: Xception: Deep learning with depthwise separable convolutions. CVPR (2017)</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/pdf/1511.07122.pdf">Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. ICLR (2016)</a></p>
<p>8.<a class="reference external" href="https://arxiv.org/pdf/1705.09914.pdf">Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. CVPR (2017)</a></p>
<p>9.<a class="reference external" href="https://arxiv.org/abs/1704.08545">Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J.: Icnet for real-time semantic segmentation on high-resolution images. arXiv preprint arXiv:1704.08545 (2017)</a></p>
<p>10.<a class="reference external" href="https://arxiv.org/abs/1412.1283v2">Dai, J., He, K., Sun, J.: Convolutional feature masking for joint object and stuff segmentation.In: CVPR. (2015)</a></p>
<p>11.<a class="reference external" href="https://arxiv.org/pdf/1709.02755">Tao Lei, Yu Zhang, and Yoav Artzi. Training rnns as fast as cnns. In EMNLP, 2018. 8</a></p>
<p>12.<a class="reference external" href="https://arxiv.org/abs/1608.03983">Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. 5</a></p>
<p>13.<a class="reference external" href="https://www.mendeley.com/catalogue/7ff08f6f-384f-3129-9f54-b7327bdd4276/">Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, 2011. 6</a></p>
<p>14.<a class="reference external" href="https://ieeexplore.ieee.org/document/6126343">Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578,2016.2</a></p>
<p>15.<a class="reference external" href="https://arxiv.org/pdf/1803.02758v4">M. Siam, M. Gamal, M. Abdel-Razek, S. Yogamani, and M.Jagersand. rtseg: Real-time semantic segmentation comparative study. In 2018 25th IEEE International Conference on Image Processing (ICIP).7</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/pdf/1812.09926">Anonymous. Snas: stochastic neural architecture search. In Submitted to International Conference on Learning Repre sentations, 2019. under review.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/abs/1512.03385">K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages770–778, 2016.</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/pdf/1711.09224">G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger.Condensenet: An efficient densenet using learned group convolutions. group, 3(12):11, 2017.</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/abs/1611.01144">E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144,2016.</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1412.6980v6">D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</a></p>
<p>6.<a class="reference external" href="https://arxiv.org/abs/1802.03268">H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268, 2018.</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/pdf/1707.01083">X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arxiv 2017. arXiv preprint arXiv:1707.01083.</a></p>
<p>8.<a class="reference external" href="https://arxiv.org/abs/1706.00046v3">T. Veniat and L. Denoyer. Learning time/memory-efficient deep architectures with budgeted super networks. arXiv preprint arXiv:1706.00046, 2017.</a></p>
<p>9.<a class="reference external" href="https://arxiv.org/abs/1804.03230v1">T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze, and H. Adam. Netadapt: Platform-aware neuralnetwork adaptation for mobile applications. Energy, 41:46,2018.</a></p>
<p>10.<a class="reference external" href="https://arxiv.org/pdf/1611.05128">Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy aware pruning. arXiv preprint arXiv:1611.05128, 2016.</a></p>
<p>11.<a class="reference external" href="https://arxiv.org/abs/1707.07012v1">Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le. Learning transferable architectures for scalable image recognition. pages 8697–8710, 06 2018</a></p>
<p>12.<a class="reference external" href="https://arxiv.org/pdf/1710.09412">HongyiZhang, MoustaphaCisse, YannNDauphin, andDavid Lopez-Paz. mixup: Beyond empirical risk minimization.ICLR, 2018. 5</a></p>
<p>13.<a class="reference external" href="https://arxiv.org/pdf/1807.11164">NingningMa,XiangyuZhang,Hai-TaoZheng, andJianSun.ShuffleNet V2: Practical guidelines for efficient CNN architecture design. arXiv preprint arXiv:1807.11164, 2018.</a></p>
<p>14.<a class="reference external" href="https://arxiv.org/abs/1905.11946v5">Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.</a></p>
<p>15.<a class="reference external" href="https://arxiv.org/abs/1807.11626">M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le.Mnasnet: Platform-aware neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.</a></p>
<p>16.<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5206848"> Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database.In CVPR, 2009. 5</a></p>
<p>17.<a class="reference external" href="https://arxiv.org/abs/2103.06877">Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and accurate model scaling. arXiv preprint arXiv:2103.06877, 2021. 12</a></p>
<p>18.<a class="reference external" href="http://export.arxiv.org/abs/1806.09055">Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts:Differentiable architecture search. ICLR, 2019. 3</a></p>
<p>19.<a class="reference external" href="https://arxiv.longhoe.net/abs/1912.09640">Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie  Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,Yang, AlanYuille, and Jianchao Yang. Atomnas: Fine-grained end-to-end neural architecture search. ICLR, 2020. 7</a></p>
<p>20.<a class="reference external" href="https://arxiv.longhoe.net/abs/1805.07440">Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian,and Rodrigo Fonseca. Neural architecture search using deep neural networks and monte carlo tree search. In AAAI, 2020.</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/abs/1608.06993v2">Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger,K. Q. Densely connected convolutional networks. CVPR,2017.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/pdf/1805.08974">Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet models transfer better? CVPR, 2019.</a></p>
<p>3.<a class="reference external" href="https://core.ac.uk/display/21817232">Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical Report, 2009.</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/pdf/1806.10909">Lin, H. and Jegelka, S. Resnet with one-neuron hidden layers is a universal approximator. NeurIPS, pp. 6172 6181, 2018.</a></p>
<p>5.<a class="reference external" href="https://arxiv.longhoe.net/abs/1807.11164">Ma, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufflenet v2:Practical guidelines for efficient cnn architecture design.ECCV, 2018.</a></p>
<p>6.<a class="reference external" href="https://store.computer.org/csdl/proceedings-article/cvpr/2018/642000i697/17D45Xtvpcc">Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning transferable architectures for scalable image recognition.CVPR, 2018.</a></p>
<p>7.<a class="reference external" href="https://arxiv.longhoe.net/abs/1605.07146">Zagoruyko, S. and Komodakis, N. Wide residual networks.BMVC, 2016.</a></p>
<p>8.<a class="reference external" href="https://arxiv.longhoe.net/abs/1512.04150">Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,A. Learning deep features for discriminative localization.CVPR, pp. 2921–2929, 2016.</a></p>
<p>9.<a class="reference external" href="https://arxiv.longhoe.net/abs/1710.10196">Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation. ICLR, 2018.</a></p>
<p>10.<a class="reference external" href="https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf">Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical Report, 2009.</a></p>
<p>11.<a class="reference external" href="https://arxiv.longhoe.net/abs/1911.04252">Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self  training with noisy student improves imagenet classification. CVPR, 2020.</a></p>
<p>12.<a class="reference external" href="https://arxiv.longhoe.net/abs/1710.09412">Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.Mixup: Beyond empirical risk minimization. ICLR, 2018.</a></p>
<p>13.<a class="reference external" href="https://arxiv.longhoe.net/abs/2003.13630"> Ridnik, T., Lawen, H., Noy, A., Baruch, E. B., Sharir,G., and Friedman, I. Tresnet: High performance gpu dedicated architecture. arXiv preprint arXiv:2003.13630,2020.</a></p>
<p>14.<a class="reference external" href="https://arxiv.longhoe.net/abs/1801.04381">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR, 2018.</a></p>
<p>15.<a class="reference external" href="https://arxiv.longhoe.net/abs/1709.02540">Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expressive power of neural networks:   A view from the width.NeurIPS, 2018. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L.The expressive power of neural networks: A view from the width.NeurIPS, 2018.</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/abs/1812.00332">Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. In ICLR, 2019.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/abs/1904.01186">Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang,Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In ICCV, 2019.</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/abs/1412.7062v4">Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2016.</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/pdf/1908.03835v1">Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang Wang. Autogan: Neural architecture search for generative adversarial networks. In ICCV, 2019.</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/pdf/1901.00392">Kai Han, Jianyuan Guo, Chao Zhang, and Mingjian Zhu.Attribute-aware attention model for fine-grained representation learning. In ACM MM, 2018.</a></p>
<p>6.<a class="reference external" href="https://arxiv.org/abs/1612.03144v1">Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017.</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/pdf/1810.05270">Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. In ICLR, 2019.</a></p>
<p>8.<a class="reference external" href="https://arxiv.org/abs/1909.07378">Mingzhu Shen, Kai Han, Chunjing Xu, and Yunhe Wang. Searching for accurate binary neural architectures. In ICCV Workshops, 2019.</a></p>
<p>9.<a class="reference external" href="https://arxiv.org/pdf/1707.07012">Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, pages 8697–8710, 2018.</a></p>
<p>10.<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3097983.3098135">Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In SIGKDD, 2017.</a></p>
<p>11.<a class="reference external" href="https://arxiv.org/abs/2010.11929v1">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</a></p>
<p>12.<a class="reference external" href="https://arxiv.org/pdf/1709.01507">Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132–7141, 2018.</a></p>
<p>13.<a class="reference external" href="https://arxiv.org/abs/1807.11164">Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV),pages 116–131, 2018.</a></p>
<p>14.<a class="reference external" href="https://arxiv.org/pdf/2110.02178.pdf">Sachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021.</a></p>
<p>15.<a class="reference external" href="https://arxiv.org/abs/1511.06067">Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/abs/1904.09925">Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional networks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3286–3295, 2019.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/abs/2103.14899">Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossVit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021a.</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/abs/1706.05587v3">Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/abs/2108.05895v3">Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. arXiv preprint arXiv:2108.05895,2021b.</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1610.02357">Franc ¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.</a></p>
<p>6.<a class="reference external" href="https://arxiv.org/abs/1805.09501">Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019.</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/abs/2106.04803">Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. arXiv preprint arXiv:2106.04803, 2021.</a></p>
<p>8.<a class="reference external" href="https://arxiv.org/pdf/2103.10697">St´ ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697, 2021.</a></p>
<p>9.<a class="reference external" href="https://arxiv.longhoe.net/abs/2010.11929">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.</a></p>
<p>10.<a class="reference external" href="https://arxiv.org/abs/1905.11946">Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net works. In International conference on machine learning, pp. 6105–6114. PMLR, 2019.</a></p>
<p>11.<a class="reference external" href="https://arxiv.org/abs/2106.14881v1">Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll´ar, and Ross Girshick. Early convolutions help transformers see better. Advances in Neural Information Processing Systems, 34:30392–30400, 2021.</a></p>
<p>12.<a class="reference external" href="https://arxiv.org/abs/2106.03348">Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. Advances in Neural Information Processing Systems, 34:28522 28535, 2021b.</a></p>
<p>13.<a class="reference external" href="https://arxiv.org/abs/2105.13677">Qinglong Zhang and Yu-Bin Yang. Rest: An efficient transformer for visual recognition. Advances in Neural Information Processing Systems, 34:15475–15485, 2021.</a></p>
<p>14.<a class="reference external" href="https://arxiv.org/abs/2103.14030">Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012–10022, 2021.</a></p>
<p>15.<a class="reference external" href="https://arxiv.org/abs/1904.10509">Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.</a></p>
<p>16.<a class="reference external" href="https://arxiv.org/abs/2001.04451v2">Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.</a></p>
<p>17.<a class="reference external" href="https://arxiv.org/abs/2007.04825v1">Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention.Advances in Neural Information Processing Systems, 33:21665–21674, 2020.</a></p>
<p>18.<a class="reference external" href="https://arxiv.org/pdf/1409.0473">Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</a></p>
<p>19.[Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838–855, 1992.](Acceleration of stochastic approximation by averaging)</p>
<p>20.<a class="reference external" href="https://arxiv.org/abs/2104.10972">Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.</a></p>
<p>1.<a class="reference external" href="https://arxiv.longhoe.net/abs/2005.12872">Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to end object detection with transformers. In ECCV, 2020. 2,4, 5, 7, 8</a></p>
<p>2.<a class="reference external" href="https://arxiv.longhoe.net/abs/2003.10027">Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic relu. In ECCV,2020. 2, 3, 4, 6</a></p>
<p>3.<a class="reference external" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html">Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va sudevan, and Quoc V. Le. Autoaugment: Learning augmen tation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), June 2019. 5</a></p>
<p>4.<a class="reference external" href="https://arxiv.longhoe.net/abs/2103.10697">St´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases.arXiv preprint arXiv:2103.10697, 2021. 2, 3, 5, 6</a></p>
<p>5.<a class="reference external" href="https://store.computer.org/csdl/proceedings-article/cvpr/2009/05206848/12OmNxWcH55">Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5, 6, 12</a></p>
<p>6.<a class="reference external" href="https://arxiv.longhoe.net/abs/2107.00652">Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021. 2</a></p>
<p>7.<a class="reference external" href="https://arxiv.longhoe.net/abs/2010.11929">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 1, 2, 3</a></p>
<p>8.<a class="reference external" href="https://arxiv.longhoe.net/abs/2104.01136">Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,Pierre Stock, Armand Joulin, Herv´ e J´ egou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. arXiv preprint arXiv:22104.01136, 2021. 1,2, 3, 6</a></p>
<p>9.<a class="reference external" href="https://arxiv.longhoe.net/abs/1512.03385">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and patter recognition, pages 770–778, 2016. 3, 7, 8</a></p>
<p>10.<a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2021arXiv210212627H/abstract">Geoffrey E. Hinton. How to represent part-whole hierarchies in a neural network. CoRR, abs/2102.12627, 2021. 2</a></p>
<p>11.<a class="reference external" href="https://arxiv.longhoe.net/abs/1905.02244">Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for mobilenetv3. In Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV), October 2019. 1, 2, 4, 5, 6, 7, 8, 11, 12</a></p>
<p>12.<a class="reference external" href="https://arxiv.longhoe.net/abs/1704.04861">Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An dreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1, 2</a></p>
<p>13.<a class="reference external" href="https://arxiv.longhoe.net/abs/1709.01507">Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2</a></p>
<p>14.<a class="reference external" href="https://arxiv.longhoe.net/abs/2103.15808">Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers, 2021. 1, 2, 3</a></p>
<p>15.<a class="reference external" href="https://arxiv.longhoe.net/abs/2007.02269">DaquanZhou, Qi-BinHou, Y.Chen, Jiashi Feng, and S. Yan.Rethinking bottleneck structure for efficient mobile network design. In ECCV, August 2020. 2</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/abs/1711.05101v3">Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6, 8</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/abs/2204.07118">Hugo Touvron, Matthieu Cord, and Herve J ´ egou. Deit iii: ´Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.13</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/abs/2205.13213">Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention. arXiv preprint arXiv:2205.13213, 2022. 1</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/abs/2003.02436v1">Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, and Le Hou. Talking-heads attention. arXiv preprint arXiv:2003.02436, 2020. 4</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/2205.12956">Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng Yan. Inception transformer. arXiv preprint arXiv:2205.12956, 2022. 1, 2, 4</a></p>
<p>6.<a class="reference external" href="https://arxiv.org/pdf/2204.05525">Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen,Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen.Topformer: Token pyramid transformer for mobile semantic segmentation, 2022. 2</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/abs/2105.12723">Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Arik, and Tomas Pfister. Nested hierarchical transformer:Towards accurate, data-efficient and interpretable visual understanding. 2022. 2</a></p>
<p>8.<a class="reference external" href="https://arxiv.org/abs/2111.11418">Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng,and Shuicheng Yan. Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418, 2021</a></p>
<p>9.<a class="reference external" href="https://arxiv.org/pdf/2111.12763">Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers. Advances in Neural Information Processing Systems, 34:9895–9907, 2021.</a></p>
<p>10.<a class="reference external" href="https://arxiv.org/abs/2110.02178">Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021.</a></p>
<p>1.<a class="reference external" href="https://arxiv.org/abs/1606.01561v1">Khalid Ashraf, Bichen Wu, Forrest N. Iandola, Matthew W. Moskewicz, and Kurt Keutzer. Shallow networks for high-accuracy road object-detection. arXiv:1606.01561, 2016.</a></p>
<p>2.<a class="reference external" href="https://arxiv.org/pdf/1807.10221v1.pdf">Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. SegNet: A deep convolutional encoderdecoder architecture for image segmentation. arxiv:1511.00561, 2015.</a></p>
<p>3.<a class="reference external" href="https://arxiv.org/abs/1512.01274">Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv:1512.01274, 2015a.</a></p>
<p>4.<a class="reference external" href="https://arxiv.org/abs/1310.1531v1">Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition.
arXiv:1310.1531, 2013.</a></p>
<p>5.<a class="reference external" href="https://arxiv.org/abs/1607.04381v1">Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. arXiv:1607.04381, 2016b</a></p>
<p>6.<a class="reference external" href="https://ieeexplore.ieee.org/document/5981829/">C. Farabet, B. Martini, B. Corda, P. Akselrod, E. Culurciello, and Y. LeCun. Neuflow: A runtime reconfigurable dataflow processor for vision. In Computer Vision and Pattern Recognition Workshops (CVPRW),2011 IEEE Computer Society Conference on, pages109–116, 2011.</a></p>
<p>7.<a class="reference external" href="https://arxiv.org/pdf/1405.3866.pdf">M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.</a></p>
<p>8.<a class="reference external" href="http://allenai.org/plato/xnornet">M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525–542, 2016.</a></p>
<p>9.<a class="reference external" href="https://dl.acm.org/doi/10.1145/1498765.1498785"> S. Williams, A. Waterman, and D. Patterson. Roofline:an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):65–76, 2009.</a></p>
<p>10.<a class="reference external" href="https://arxiv.org/abs/1710.07368">B. Wu, A. Wan, X. Yue, and K. Keutzer. Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud. arXiv preprint arXiv:1710.07368, 2017.</a></p>
<p>11.<a class="reference external" href="https://arxiv.org/abs/1409.1556">K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556, 2014.</a></p>
<p>12.<a class="reference external" href="https://arxiv.org/abs/1512.03385">K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.</a></p>
<p>13.<a class="reference external" href="https://arxiv.org/abs/1502.03167v3">S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine
learning, pages 448–456, 2015.</a></p>
<p>14.<a class="reference external" href="https://arxiv.org/pdf/1510.00149.pdf">S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations(ICLR), 2016.</a></p>
<p>15.<a class="reference external" href="https://arxiv.org/pdf/1704.04861.pdf">A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko,W. Wang, T. Weyand, M. Andreetto, and H. Adam.Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.</a></p>
<ul class="simple">
<li><p>Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization</p></li>
<li><p>Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks（ICCV 2019）</p></li>
<li><p>IR-Net: Forward and Backward Information Retention for Highly Accurate Binary Neural Networks（CVPR 2020）</p></li>
<li><p>Towards Unified INT8 Training for Convolutional Neural Network（CVPR 2020）</p></li>
<li><p>Rotation Consistent Margin Loss for Efficient Low-bit Face Recognition（CVPR 2020）</p></li>
<li><p>DMS: Differentiable diMension Search for Binary Neural Networks（ICLR 2020 Workshop）</p></li>
<li><p>Nagel, Markus, et al. &quot;A white paper on neural network quantization.&quot; arXiv preprint arXiv:2106.08295 (2021).</p></li>
<li><p>Krishnamoorthi, Raghuraman. &quot;Quantizing deep convolutional networks for efficient inference: A whitepaper.&quot; arXiv preprint arXiv:1806.08342 (2018)</p></li>
<li><p>全网最全-网络模型低比特量化 https://zhuanlan.zhihu.com/p/453992336</p></li>
<li><p><a class="reference external" href="https://pytorch.org/blog/quantization-in-practice/">Practical Quantization in PyTorch</a></p></li>
<li><p>Jacob, Benoit, et al. &quot;Quantization and training of neural networks for efficient integer-arithmetic-only inference.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</p></li>
<li><p>Wu, Hao, et al. &quot;Integer quantization for deep learning inference: Principles and empirical evaluation.&quot; arXiv preprint arXiv:2004.09602 (2020).</p></li>
<li><p>Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization</p></li>
<li><p>Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks（ICCV 2019）</p></li>
<li><p>IR-Net: Forward and Backward Information Retention for Highly Accurate Binary Neural Networks（CVPR 2020）</p></li>
<li><p>Towards Unified INT8 Training for Convolutional Neural Network（CVPR 2020）</p></li>
<li><p>Rotation Consistent Margin Loss for Efficient Low-bit Face Recognition（CVPR 2020）</p></li>
<li><p>DMS: Differentiable diMension Search for Binary Neural Networks（ICLR 2020 Workshop）</p></li>
<li><p>Nagel, Markus, et al. &quot;A white paper on neural network quantization.&quot; arXiv preprint arXiv:2106.08295 (2021).</p></li>
<li><p>Krishnamoorthi, Raghuraman. &quot;Quantizing deep convolutional networks for efficient inference: A whitepaper.&quot; arXiv preprint arXiv:1806.08342 (2018)</p></li>
<li><p>Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., &amp; Keutzer, K. (2021). A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630.</p></li>
<li><p>Wu, H., Judd, P., Zhang, X., Isaev, M., &amp; Micikevicius, P. (2020). Integer quantization for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602.</p></li>
<li><p><a class="reference external" href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">8-bit Inference with TensorRT </a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/blog/quantization-in-practice/">Practical Quantization in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/453992336">全网最全-网络模型低比特量化</a></p></li>
</ul>
<ol class="arabic simple">
<li><p>Jianping Gou et al. Knowledge Distillation: A Survey. https://doi.org/10.1007/s11263-021-01453-z</p></li>
<li><p>Hinton et al. Distilling the Knowledge in a Neural Network. http://arxiv.org/abs/1503.02531</p></li>
<li><p>Longhui Wei et al. Circumventing outlier of autoaugment with knowledge distillation.  https://doi.org/10.1007/978-3-030-58580-8_36</p></li>
<li><p>Caruana et al. Model compression. https://doi.org/10.1145/1150402.1150464</p></li>
<li><p>模型压缩（上）--知识蒸馏（Distilling Knowledge）https://www.jianshu.com/p/a6d87b338bcf</p></li>
<li><p>DeiT：注意力也能蒸馏 https://www.cnblogs.com/ZOMI/p/16496326.html</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/396781295">AI 框架部署方案之模型转换</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/658734035">AI 技术方案（个人总结）</a></p></li>
<li><p><a class="reference external" href="https://microsoft.github.io/AI-System/SystemforAI-9-Compilation%20and%20Optimization.pdf">人工智能系统 System for AI   课程介绍 Lecture Introduction</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_45651194/article/details/132921090">【AI】推理引擎的模型转换模块</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/535729752">Pytorch 和 TensorFlow 在 padding 实现上的区别</a></p></li>
<li><p><a class="reference external" href="https://openmlsys.github.io/chapter_model_deployment/model_converter_and_optimizer.html">训练模型到推理模型的转换及优化</a></p></li>
<li><p><a class="reference external" href="https://www.TensorFlow.org/guide/graph_optimization?hl=zh-cn">使用 Grappler 优化 TensorFlow 计算图</a></p></li>
<li><p><a class="reference external" href="https://decaf-lang.gitbook.io/decaf-book/rust-kuang-jia-fen-jie-duan-zhi-dao/pa4-zhong-jian-dai-ma-you-hua/si-dai-ma-xiao-chu">死代码消除</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/599949051">AI 编译器之前端优化-下（笔记）</a></p></li>
<li><p><a class="reference external" href="https://pytorch123.com/ThirdSection/SaveModel/">PyTorch 官方教程中文版</a></p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorial/zh-CN/r1.2/save_load_model.html">MindSpore 教程</a></p></li>
<li><p><a class="reference external" href="https://www.TensorFlow.org/tutorials/keras/save_and_load?hl=zh-cn">TensorFlow Core</a></p></li>
<li><p><a class="reference external" href="https://www.TensorFlow.org/guide/keras/save_and_serialize?hl=zh-cn">保存和加载 Keras 模型</a></p></li>
<li><p><a class="reference external" href="https://cloud.baidu.com/article/3251524">探索 ONNX 模型：动态输入尺寸的实践与解决方案</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_43863869/article/details/128638397">Pytorch 复习笔记--导出 Onnx 模型为动态输入和静态输入</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_46649052/article/details/119763872">PyTorch 学习—19.模型的加载与保存（序列化与反序列化）</a></p></li>
<li><p><a class="reference external" href="https://github.com/aipredict/ai-models-serialization">开源 AI 模型序列化总结</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/346511883">ONNX 学习笔记</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/volvet/article/details/85013830">深入 CoreML 模型定义</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;JMangiaswift-loves-TensorFlow-and-coreml-2a11da25d44">Swift loves TensorFlow and CoreML</a></p></li>
<li><p><a class="reference external" href="https://blog.postman.com/what-is-protobuf/">什么是 Protobuf？</a></p></li>
<li><p><a class="reference external" href="https://colobu.com/2015/01/07/Protobuf-language-guide/">Protobuf 语法指南</a></p></li>
<li><p><a class="reference external" href="https://halfrost.com/flatbuffers_schema/">深入浅出 FlatBuffers 之 Schema</a></p></li>
<li><p><a class="reference external" href="https://www.jianshu.com/p/8eb153c12a4b">FlatBuffers，MNN 模型存储结构基础 ---- 无法解读 MNN 模型文件的秘密</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/m0_37605642/article/details/125691987">华为昇思 MindSpore 详细教程（一）</a></p></li>
<li><p><a class="reference external" href="https://www.volcengine.com/theme/9557712-R-7-1">如何将在 GPU 上训练的模型加载到 CPU（系统）内存中？</a></p></li>
<li><p><a class="reference external" href="http://121.199.45.168:13007/01-PyTorch%E4%BD%BF%E7%94%A8/11-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD.html">11 模型的保存加载¶</a></p></li>
</ol>
<ul class="simple">
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/document/726791">Lecun Y , Bottou L .Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86(11):2278-2324.DOI:10.1109/5.726791.</a></p></li>
<li><p><a class="reference external" href="https://www.semanticscholar.org/paper/Neocognitron%3A-A-Self-Organizing-Neural-Network-for-Fukushima-Miyake/9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5">Fukushima, Kunihiko and Sei Miyake. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition.” (1982).</a></p></li>
<li><p><a class="reference external" href="https://www.semanticscholar.org/paper/Notes-on-Convolutional-Neural-Networks-Bouvrie/2a4393aa1bc3cb7fe2deecc88720bfb84dabb263"> Bouvrie J .Notes on Convolutional Neural Networks[J].neural nets, 2006.</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/10.1145/3065386">Krizhevsky A , Sutskever I , Hinton G .ImageNet Classification with Deep Convolutional Neural Networks[J].Advances in neural information processing systems, 2012, 25(2).DOI:10.1145/3065386.</a></p></li>
<li><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. Deep Learning. MIT Press, 2016.</p></li>
<li><p><a class="reference external" href="https://zhenhuaw.me/blog/2019/convolution-neural-networks-optimization.html">卷积神经网络优化算法</a></p></li>
</ul>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://epubs.siam.org/doi/book/10.1137/1.9781611970364">Winograd, Shmuel. Arithmetic complexity of computations. Vol. 33. Siam, 1980.</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1509.09308">Lavin, Andrew, and Scott Gray. &quot;Fast algorithms for convolutional neural networks.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</a></p></li>
<li><p><a class="reference external" href="https://github.com/andravin/wincnn">A simple python module for computing minimal Winograd convolution algorithms for use with convolutional neural networks</a></p></li>
<li><p><a class="reference external" href="https://www.bilibili.com/video/av50718398">video: Fast Algorithms for Convolutional Neural Networks by Andrew Lavin and Scott Gray</a></p></li>
<li><p><a class="reference external" href="https://www.bilibili.com/video/av53072685">video: Even Faster CNNs Exploring the New Class of Winograd Algorithms</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;dmangla3/understanding-winograd-fast-convolution-a75458744ff">Understanding ‘Winograd Fast Convolution’</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/260109670">详解卷积中的 Winograd 加速算法</a></p></li>
<li><p><a class="reference external" href="https://juejin.cn/post/7061244517789368333">一文看懂 winograd 卷积加速算法</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/102351953">详解 Winograd 变换矩阵生成原理</a></p></li>
<li><p><a class="reference external" href="https://no5-aaron-wu.github.io/2021/11/16/AI-Algorithm-4-Winograd/">AI 算法基础 [4]：Winograd 算法原理</a></p></li>
<li><p><a class="reference external" href="https://martin20150405.github.io/2019/11/13/dl-winograd-kuai-su-juan-ji-suan-fa/">[DL]Winograd 快速卷积算法</a></p></li>
<li><p><a class="reference external" href="https://www.cnblogs.com/megengine/p/16405753.html">MegEngine Inference 卷积优化之 Im2col 和 winograd 优化</a></p></li>
<li><p><a class="reference external" href="https://ajz34.readthedocs.io/zh-cn/latest/ML_Notes/winograd6x3/cnn_winograd.html">Winograd 卷积的纯 Python 实现</a></p></li>
<li><p><a class="reference external" href="https://zhenhuaw.me/blog/2019/convolution-neural-networks-optimization.html#winograd-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95c">Winograd 优化算法</a></p></li>
</ol>
</section>
<section id="id4">
<h2>五. AI 框架核心模块<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://developer.baidu.com/article/details/3129186">深入浅出：AI 框架与计算图的关系</a></p></li>
<li><p><a class="reference external" href="https://openmlsys.github.io/chapter_computational_graph/background_and_functionality.html#id1">4.1. 计算图的设计背景和作用</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_45651194/article/details/132872588">【AI】推理系统和推理引擎的整体架构</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/149464086">谈谈深度学习框架的数据排布</a></p></li>
<li><p><a class="reference external" href="https://github.com/lucasjinreal/AI-Infer-Engine-From-Zero">从零构建 AI 推理引擎系列</a></p></li>
<li><p><a class="reference external" href="https://developer.aliyun.com/article/995926">一篇就够：高性能推理引擎理论与实践 (TensorRT)</a></p></li>
<li><p><a class="reference external" href="https://harmonyhu.com/2018/08/11/flatbuffers/">序列化之 FlatBuffers</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/weixin_45651194/article/details/132921090">【AI】推理引擎的模型转换模块</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/jasonaidm/article/details/90522615">深度学习模型转换</a></p></li>
<li><p><a class="reference external" href="https://github.com/ysh329/deep-learning-model-convertor">deep-learning-model-convertor</a></p></li>
<li><p><a class="reference external" href="https://developer.horizon.ai/api/v1/fileData/doc/ddk_doc/navigation/ai_toolchain/docs_cn/hb_mapper_tools_guide/01_model_conversion_details.html">hb_mapper_tools_guide</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/363317178">模型转换：由 Pytorch 到 TFlite</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/396781295">AI 框架部署方案之模型转换</a></p></li>
<li><p><a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/IR.md">Open Neural Network Exchange Intermediate Representation (ONNX IR) Specification</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/477743341">模型部署入门教程（一）：模型部署简介</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/498425043">模型部署入门教程（三）：PyTorch 转 ONNX 详解</a></p></li>
</ol>
<p>[1] Boris Ginsburg, Sergei Nikolaev, Paulius Micikevicius, (2017). TRAINING WITH MIXED PRECISION. Retrieved from https://on-demand.GPUtechconf.com/gtc/2017/presentation/s7218-training-with-mixed-precision-boris-ginsburg.pdf.</p>
<p>[2] Wickipedia. Half-precision floating-point format. Retrieved from https://en.wikipedia.org/wiki/Half-precision_floating-point_format.</p>
<p>[3] The huggingface Authors. (2024). Methods and tools for efficient training on a single GPU. Retrieved from https://huggingface.co/docs/transformers/main/en/perf_train_GPU_one</p>
<p>[1] Li S, Zhao Y, Varma R, et al. Pytorch distributed: Experiences on accelerating data parallel training[J]. arXiv preprint arXiv:2006.15704, 2020.</p>
<p>[2] Rajbhandari S, Rasley J, Ruwase O, et al. Zero: Memory optimizations toward training trillion parameter models[C]//SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020: 1-16.</p>
<p>[3] Li M, Zhou L, Yang Z, et al. Parameter server for distributed machine learning[C]//Big learning NIPS workshop. 2013, 6(2).</p>
<p>[1] The Pytorch Authors. (2024). Getting Started with Distributed Data Parallel. Retrieved from https://pytorch.org/tutorials/intermediate/ddp_tutorial.html.</p>
<p>[1] Rajbhandari S, Rasley J, Ruwase O, et al. Zero: Memory optimizations toward training trillion parameter models[C]//SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020: 1-16.</p>
<p>[2] Rajbhandari S, Ruwase O, Rasley J, et al. Zero-infinity: Breaking the GPU memory wall for extreme scale deep learning[C]//Proceedings of the international conference for high performance computing, networking, storage and analysis. 2021: 1-14.</p>
<p>[3] Lv K, Yang Y, Liu T, et al. Full parameter fine-tuning for large language models with limited resources[J]. arXiv preprint arXiv:2306.09782, 2023.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./00Others"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
<div class="section ablog__blog_comments">
  
  
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Install.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">本地部署(DONE)</p>
      </div>
    </a>
    <a class="right-next"
       href="Glossary.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">术语表(DONE)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">一. AI 系统概述</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">二. AI 硬件体系结构</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">三. AI 编译器</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">四. 推理系统&amp;引擎</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">五. AI 框架核心模块</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Oct 09, 2024.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>