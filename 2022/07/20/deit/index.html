

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ZOMI酱">
  <meta name="keywords" content="">
  
    <meta name="description" content="《Training data-efﬁcient image transformers &amp; distillation through attention》 ViT 在大数据集 ImageNet-21k（14million）或者 JFT-300M（300million） 上进行训练，Batch Size 128 下 NVIDIA A100 32G GPU 的计算资源加持下预训练 ViT-Bas">
<meta property="og:type" content="article">
<meta property="og:title" content="DeiT：注意力Attention也能蒸馏">
<meta property="og:url" content="http://example.com/2022/07/20/deit/index.html">
<meta property="og:site_name" content="ZOMI酱的博客">
<meta property="og:description" content="《Training data-efﬁcient image transformers &amp; distillation through attention》 ViT 在大数据集 ImageNet-21k（14million）或者 JFT-300M（300million） 上进行训练，Batch Size 128 下 NVIDIA A100 32G GPU 的计算资源加持下预训练 ViT-Bas">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/deit/experience1.png">
<meta property="article:published_time" content="2022-07-20T01:17:33.000Z">
<meta property="article:modified_time" content="2022-07-20T12:08:50.121Z">
<meta property="article:author" content="ZOMI酱">
<meta property="article:tag" content="DeiT">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/images/deit/experience1.png">
  
  
  
  <title>DeiT：注意力Attention也能蒸馏 - ZOMI酱的博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"8By2Rh3UdwUFi0qNVg4Fv3Vw-gzGzoHsz","app_key":"8hmRi2M643gQvVkbyzQpQk4I","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ZOMI酱</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="DeiT：注意力Attention也能蒸馏"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-20 09:17" pubdate>
          2022年7月20日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          59 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">DeiT：注意力Attention也能蒸馏</h1>
            
            
              <div class="markdown-body">
                
                <p>《Training data-efﬁcient image transformers &amp; distillation through attention》</p>
<p>ViT 在大数据集 ImageNet-21k（14million）或者 JFT-300M（300million） 上进行训练，Batch Size 128 下 NVIDIA A100 32G GPU 的计算资源加持下预训练 ViT-Base&#x2F;32 需要3天时间。</p>
<p>Facebook 与索邦大学 Matthieu Cord 教授合作发表 Training data-efficient image transformers（DeiT） &amp; distillation through attention，DeiT 模型（8600万参数）仅用一台 GPU 服务器在 53 hours train，20 hours finetune，仅使用 ImageNet 就达到了 84.2 top-1 准确性，而无需使用任何外部数据进行训练。性能与最先进的卷积神经网络（CNN）可以抗衡。所以呢，很有必要讲讲这个 DeiT 网络模型的相关内容。</p>
<p>下面来简单总结 DeiT：</p>
<blockquote>
<p>DeiT 是一个全 Transformer 的架构。其核心是提出了针对 ViT 的教师-学生蒸馏训练策略，并提出了 token-based distillation 方法，使得 Transformer 在视觉领域训练得又快又好。</p>
</blockquote>
<p><img src="/images/deit/experience1.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="DeiT-相关背景"><a href="#DeiT-相关背景" class="headerlink" title="DeiT 相关背景"></a>DeiT 相关背景</h1><p>ViT 文中表示数据量不足会导致 ViT 效果变差。针对以上问题，DeiT 核心共享是使用了蒸馏策略，能够仅使用 ImageNet-1K 数据集就就可以达到 83.1% 的 Top1。</p>
<p>那么文章主要贡献可以总结为三点：</p>
<ol>
<li>仅使用 Transformer，不引入 Conv 的情况下也能达到 SOTA 效果。</li>
<li>提出了基于 token 蒸馏的策略，针对 Transformer 蒸馏方法超越传统蒸馏方法。</li>
<li>DeiT 发现使用 Convnet 作为教师网络能够比使用 Transformer 架构效果更好。</li>
</ol>
<p>正式了解 DeiT 算法之前呢，有几个问题需要去了解的：ViT的缺点和局限性，为什么训练ViT要准备这么多数据，就不能简单快速训练一个模型出来吗？另外 Transformer 视觉模型又怎么玩蒸馏呢？</p>
<h2 id="ViT-的缺点和局限性"><a href="#ViT-的缺点和局限性" class="headerlink" title="ViT 的缺点和局限性"></a>ViT 的缺点和局限性</h2><p>Transformer的输入是一个序列（Sequence），ViT 所采用的思路是把图像分块（patches），然后把每一块视为一个向量（vector），所有的向量并在一起就成为了一个序列（Sequence），ViT 使用的数据集包括了一个巨大的包含了 300 million images的 JFT-300，这个数据集是私有的，即外部研究者无法复现实验。而且在ViT的实验中作者明确地提到：</p>
<blockquote>
<p>“That transformers do not generalize well when trained on insufficient amounts of data.”</p>
</blockquote>
<p><img src="/images/deit/transformer1.png" srcset="/img/loading.gif" lazyload alt="ViT"></p>
<p>意思是当不使用 JFT-300 大数据集时，效果不如CNN模型。也就反映出Transformer结构若想取得理想的性能和泛化能力就需要这样大的数据集。DeiT 作者通过所提出的蒸馏的训练方案，只在 Imagenet 上进行训练，就产生了一个有竞争力的无卷积 Transformer。</p>
<h3 id="Visual-transformer"><a href="#Visual-transformer" class="headerlink" title="Visual transformer"></a>Visual transformer</h3><p><strong>Multi-head Self Attention layers (MSA)：</strong></p>
<p>首先有一个 Query 矩阵 Q 和一个 Key 矩阵 K，把二者矩阵乘在一起并进行归一化以后得到 attention 矩阵，它再与Value矩阵 V 相乘得到最终的输出得到 Z。最后经过 linear transformation 得到 NxD 的输出结果。</p>
<p><img src="/images/deit/transformer2.png" srcset="/img/loading.gif" lazyload alt="Multi-head Self Attention layers (MSA)"></p>
<p><strong>Feed-Forward Network (FFN)：</strong></p>
<p>Multi-head Self Attention layers 之后往往会跟上一个 Feed-Forward Network (FFN) ，它一般是由2个linear layer构成，第1个linear layer把维度从 D 维变换到 ND 维，第2个linear layer把维度从 ND 维再变换到 D 维。</p>
<p>此时 Transformer block 是不考虑位置信息的，基于此 ViT 加入了位置编码 (Positional Encoding)，这些编码在第一个 block 之前被添加到 input token 中代表位置信息，作为额外可学习的embedding（Exgra learnable class embedding）。</p>
<p><strong>Class token：</strong></p>
<p>Class token 与 input token 并在一起输入 Transformer block 中，最后的输出结果用来预测类别。这样一来，Transformer 相当于一共处理了 N+1 个维度为 D 的token，并且只有第一个 token 的输出用来预测类别。</p>
<h2 id="知识蒸馏介绍"><a href="#知识蒸馏介绍" class="headerlink" title="知识蒸馏介绍"></a>知识蒸馏介绍</h2><p>Knowledge Distillation（KD）最初被 Hinton 提出 “Distilling the Knowledge in a Neural Network”，与 Label smoothing 动机类似，但是 KD 生成 soft label 的方式是通过教师网络得到的。</p>
<p>KD 可以视为将教师网络学到的信息压缩到学生网络中。还有一些工作 “Circumventing outlier of autoaugment with knowledge distillation” 则将 KD 视为数据增强方法的一种。</p>
<h3 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性。在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</p>
<ul>
<li>推理速度和性能慢</li>
<li>对部署资源要求高(内存，显存等)</li>
</ul>
<p>在部署时，对延迟以及计算资源都有着严格的限制。因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题，而“模型蒸馏”属于模型压缩的一种方法。</p>
<h3 id="理论原理"><a href="#理论原理" class="headerlink" title="理论原理"></a>理论原理</h3><p>知识蒸馏使用的是 Teacher—Student 模型，其中 Teacher 是“知识”的输出者，Student 是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p>
<ol>
<li><strong>原始模型训练</strong>: 训练 “Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li>
<li><strong>精简模型训练</strong>: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li>
</ol>
<p>论文中，Hinton 将问题限定在分类问题下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。知识蒸馏时，由于已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p>
<p>其中KD的训练过程和传统的训练过程的对比：</p>
<ol>
<li>传统training过程 <strong>Hard Targets</strong>: 对 ground truth 求极大似然 Softmax 值。</li>
<li>KD的training过程 <strong>Soft Targets</strong>: 用 Teacher 模型的 class probabilities作为soft targets。</li>
</ol>
<p><img src="/images/deit/kd1.png" srcset="/img/loading.gif" lazyload></p>
<p>这就解释了为什么通过蒸馏的方法训练出的 Net-S 相比使用完全相同的模型结构和训练数据只使用Hard Targets的训练方法得到的模型，拥有更好的泛化能力。</p>
<h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p>第一步是训练Net-T；第二步是在高温 T 下，蒸馏 Net-T 的知识到 Net-S。</p>
<p><img src="/images/deit/kd2.png" srcset="/img/loading.gif" lazyload></p>
<p>训练 Net-T 的过程很简单，而高温蒸馏过程的目标函数由distill loss（对应soft target）和student loss（对应hard target）加权得到：</p>
<p>$$<br>L&#x3D;\alpha L_{soft}+\beta L_{hard}<br>$$</p>
<p>Deit 中使用 Conv-Based 架构作为教师网络，以 soft 的方式将归纳偏置传递给学生模型，将局部性的假设通过蒸馏方式引入 Transformer 中，取得了不错的效果。</p>
<h1 id="DeiT-具体方法"><a href="#DeiT-具体方法" class="headerlink" title="DeiT 具体方法"></a>DeiT 具体方法</h1><p>为什么DeiT能在大幅减少 <strong>1. 训练所需的数据集</strong> 和 <strong>2. 训练时长</strong> 的情况下依旧能够取得很不错的性能呢？我们可以把这个原因归结为DeiT的训练策略。ViT 在小数据集上的性能不如使用CNN网络 EfficientNet，但是跟ViT结构相同，仅仅是使用更好的训练策略的DeiT比ViT的性能已经有了很大的提升，在此基础上，再加上蒸馏 (distillation) 操作，性能超过了 EfficientNet。</p>
<p>假设有一个性能很好的分类器作为teacher model，通过引入了一个 Distillation Token，然后在 self-attention layers 中跟 class token，patch token 在 Transformer 结构中不断学习。</p>
<p>Class token的目标是跟真实的label一致，而Distillation Token是要跟teacher model预测的label一致。</p>
<p><img src="/images/deit/deit1.png" srcset="/img/loading.gif" lazyload alt="DeiT结构"></p>
<p>对比 ViT 的输出是一个 softmax，它代表着预测结果属于各个类别的概率的分布。ViT的做法是直接将 softmax 与 GT label取 CE Loss。</p>
<p>$$<br>CELoss(x, y) &#x3D; - \sum y_i * log(x_i)<br>$$</p>
<p>而在 DeiT 中，除了 CE Loss 以外，还要 1）定义蒸馏损失；2）加上 Distillation Token。</p>
<ol>
<li><strong>定义蒸馏损失</strong></li>
</ol>
<p>蒸馏分两种，一种是软蒸馏（soft distillation），另一种是硬蒸馏（hard distillation）。软蒸馏如下式所示，Z_s 和 Z_t 分别是 student model 和 teacher model 的输出，KL 表示 KL 散度，psi 表示softmax函数，lambda 和 tau 是超参数：</p>
<p>$$<br>\mathcal{L}<em>{\text {global }}&#x3D;(1-\lambda) \mathcal{L}</em>{\mathrm{CE}}\left(\psi\left(Z_{\mathrm{s}}\right), y\right)+\lambda \tau^{2} \mathrm{KL}\left(\psi\left(Z_{\mathrm{s}} &#x2F; \tau\right), \psi\left(Z_{\mathrm{t}} &#x2F; \tau\right)\right)<br>$$</p>
<p>硬蒸馏如下式所示，其中 CE 表示交叉熵：</p>
<p>$$<br>\mathcal{L}<em>{\text {global }}^{\text {hardDistill }}&#x3D;\frac{1}{2} \mathcal{L}</em>{\mathrm{CE}}\left(\psi\left(Z_{s}\right), y\right)+\frac{1}{2} \mathcal{L}<em>{\mathrm{CE}}\left(\psi\left(Z</em>{s}\right), y_{\mathrm{t}}\right)<br>$$</p>
<p>学生网络的输出 Z_s 与真实标签之间计算 CE Loss 。如果是硬蒸馏，就再与教师网络的标签取 CE Loss。如果是软蒸馏，就再与教师网络的 softmax 输出结果取 KL Loss 。</p>
<p>值得注意的是，Hard Label 也可以通过标签平滑技术 （Label smoothing） 转换成Soft Labe，其中真值对应的标签被认为具有 1- esilon 的概率，剩余的 esilon 由剩余的类别共享。</p>
<ol start="2">
<li><strong>加入 Distillation Token</strong></li>
</ol>
<p>Distillation Token 和 ViT 中的 class token 一起加入 Transformer 中，和class token 一样通过 self-attention 与其它的 embedding 一起计算，并且在最后一层之后由网络输出。</p>
<p>而 Distillation Token 对应的这个输出的目标函数就是蒸馏损失。Distillation Token 允许模型从教师网络的输出中学习，就像在常规的蒸馏中一样，同时也作为一种对class token的补充。</p>
<p><img src="/images/deit/deit2.png" srcset="/img/loading.gif" lazyload alt="DeiT训练流程"></p>
<h1 id="DeiT-具体实验"><a href="#DeiT-具体实验" class="headerlink" title="DeiT 具体实验"></a>DeiT 具体实验</h1><p>实验参数的设置：图中表示不同大小的 DeiT 结构的超参数设置，最大的结构是 DeiT-B，与 ViT-B 结构是相同，唯一不同的是 embedding 的 hidden dimension 和 head 数量。作者保持了每个head的隐变量维度为64，throughput是一个衡量DeiT模型处理图片速度的变量，代表每秒能够处理图片的数目。</p>
<p><img src="/images/deit/experience2.png" srcset="/img/loading.gif" lazyload alt="不同大小的DeiT结构的超参数设置"></p>
<ol>
<li><strong>Teacher model对比</strong></li>
</ol>
<p>作者首先观察到使用 CNN 作为 teacher 比 transformer 作为 teacher 的性能更优。下图中对比了 teacher 网络使用 DeiT-B 和几个 CNN 模型 RegNetY 时，得到的 student 网络的预训练性能以及 finetune 之后的性能。</p>
<p>其中，DeiT-B 384 代表使用分辨率为 384×384 的图像 finetune 得到的模型，最后的那个小蒸馏符号 alembic sign 代表蒸馏以后得到的模型。</p>
<p><img src="/images/deit/experience3.png" srcset="/img/loading.gif" lazyload alt="不同teacher模型的性能指标对比"></p>
<ol start="2">
<li><strong>蒸馏方法对比</strong></li>
</ol>
<p>下图是不同蒸馏策略的性能对比，label 代表有监督学习，前3行分别是不使用蒸馏，使用soft蒸馏和使用hard蒸馏的性能对比。前3行不使用 Distillation Token 进行训练，只是相当于在原来 ViT 的基础上给损失函数加上了蒸馏部分。</p>
<p>对于Transformer来讲，硬蒸馏的性能明显优于软蒸馏，即使只使用 class token，不使用 distill token，硬蒸馏达到 83.0%，而软蒸馏的精度为 81.8%。  </p>
<p><img src="/images/deit/experience4.png" srcset="/img/loading.gif" lazyload alt="不同蒸馏策略的性能对比"></p>
<p>从最后两列 B224 和 B384 看出，以更高的分辨率进行微调有助于减少方法之间的差异。这可能是因为在微调时，作者不使用教师信息。随着微调，class token 和 Distillation Token 之间的相关性略有增加。</p>
<p>除此之外，蒸馏模型在 accuracy 和 throughput 之间的 trade-off 甚至优于 teacher 模型，这也反映了蒸馏的有趣之处。</p>
<ol start="3">
<li><strong>性能对比</strong></li>
</ol>
<p>下面是不同模型性能的数值比较。可以发现在参数量相当的情况下，卷积网络的速度更慢，这是因为大的矩阵乘法比小卷积提供了更多的优化机会。EffcientNet-B4和DeiT-B alembic sign的速度相似，在3个数据集的性能也比较接近。</p>
<p><img src="/images/deit/experience5.png" srcset="/img/loading.gif" lazyload alt="不同模型性能的数值比较"></p>
<ol start="4">
<li><strong>对比实验</strong></li>
</ol>
<p>作者还做了一些关于数据增强方法和优化器的对比实验。Transformer的训练需要大量的数据，想要在不太大的数据集上取得好性能，就需要大量的数据增强，以实现data-efficient training。几乎所有评测过的数据增强的方法都能提升性能。对于优化器来说，AdamW比SGD性能更好。</p>
<p>此外，发现Transformer对优化器的超参数很敏感，试了多组 lr 和 weight+decay。stochastic depth有利于收敛。Mixup 和 CutMix 都能提高性能。Exp.+Moving+Avg. 表示参数平滑后的模型，对性能提升只是略有帮助。最后就是 Repeated augmentation 的数据增强方式对于性能提升帮助很大。</p>
<p><img src="/images/deit/experience6.png" srcset="/img/loading.gif" lazyload alt="对比实验"></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>DeiT 模型（8600万参数）仅用一台 GPU 服务器在 53 hours train，20 hours finetune，仅使用 ImageNet 就达到了 84.2 top-1 准确性，而无需使用任何外部数据进行训练，性能与最先进的卷积神经网络（CNN）可以抗衡。其核心是提出了针对 ViT 的教师-学生蒸馏训练策略，并提出了 token-based distillation 方法，使得 Transformer 在视觉领域训练得又快又好。</p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349315675">https://zhuanlan.zhihu.com/p/349315675</a></p>
<p>[2] <a target="_blank" rel="noopener" href="http://giantpandacv.com/academic/%E7%AE%97%E6%B3%95%E7%A7%91%E6%99%AE/Transformer/DeiT%EF%BC%9A%E4%BD%BF%E7%94%A8Attention%E8%92%B8%E9%A6%8FTransformer/">DeiT：使用Attention蒸馏Transformer</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102038521">https://zhuanlan.zhihu.com/p/102038521</a></p>
<p>[4] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 2.7 (2015).</p>
<p>[5] Touvron, Hugo, et al. “Training data-efficient image transformers &amp; distillation through attention.” International Conference on Machine Learning. PMLR, 2021.</p>
<p>[6] Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</p>
<p>[7] Wei, Longhui, et al. “Circumventing outliers of autoaugment with knowledge distillation.” European Conference on Computer Vision. Springer, Cham, 2020.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/DeiT/">#DeiT</a>
      
        <a href="/tags/Transformer/">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>DeiT：注意力Attention也能蒸馏</div>
      <div>http://example.com/2022/07/20/deit/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>ZOMI酱</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月20日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/20/mocov1/" title="MoCo V1：视觉领域也能自监督啦">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">MoCo V1：视觉领域也能自监督啦</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"8By2Rh3UdwUFi0qNVg4Fv3Vw-gzGzoHsz","appKey":"8hmRi2M643gQvVkbyzQpQk4I","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
